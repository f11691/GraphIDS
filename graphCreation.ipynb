{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d67386-8119-4176-9290-4f113cfacac5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#graph creation for Trainning\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('dataset')\n",
    "\n",
    "# Convert 'Timestamp' to datetime and set as index\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "# Sort the DataFrame by the index (Timestamp)\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# Define time interval (Daily, hourly or per minutes .... )\n",
    "time_interval = 'X'\n",
    "\n",
    "# Updated required columns list based on the new features for edge attributes\n",
    "required_columns = [\n",
    "    'Src IP', 'Dst IP', 'Flow ID', 'Src Port', 'Dst Port', 'Protocol',\n",
    "    'Flow Duration', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Flow IAT Max',\n",
    "    'Flow IAT Mean', 'Flow IAT Min', 'Flow IAT Std', 'Fwd IAT Max',\n",
    "    'Fwd IAT Mean', 'Fwd IAT Min', 'Fwd IAT Std', 'Bwd IAT Max',\n",
    "    'Bwd IAT Mean', 'Bwd IAT Min', 'Bwd IAT Std', 'Active Max',\n",
    "    'Active Mean', 'Active Min', 'Active Std', 'Label'\n",
    "]\n",
    "\n",
    "# Check for missing columns in the DataFrame\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"The following required columns are missing from the DataFrame: {missing_columns}\")\n",
    "else:\n",
    "    print(\"All required columns are present in the DataFrame.\")\n",
    "\n",
    "def process_time_slice(slice_df, interval, index):\n",
    "    G = nx.DiGraph()  # Changed to directed graph to capture directionality of communication\n",
    "    try:\n",
    "        for idx, row in slice_df.iterrows():\n",
    "            if pd.isna(row['Src IP']) or pd.isna(row['Dst IP']):\n",
    "                continue\n",
    "\n",
    "            # Define the nodes connected by the edge\n",
    "            edge_id = (row['Src IP'], row['Dst IP'])\n",
    "                       \n",
    "            src_node, dst_node = row['Src IP'], row['Dst IP']\n",
    "            G.add_node(src_node, type='IP')\n",
    "            G.add_node(dst_node, type='IP')\n",
    "\n",
    "            # Define edge with attributes\n",
    "            # Network Communication Edge\n",
    "            G.add_edge(*edge_id, interaction='network_communication',\n",
    "                       flow_id=row['Flow ID'], src_port=row['Src Port'], dst_port=row['Dst Port'],\n",
    "                       protocol=row['Protocol'],\n",
    "                       fwd_psh_flags=row['Fwd PSH Flags'], bwd_psh_flags=row['Bwd PSH Flags'],\n",
    "                       flow_iat_max=row['Flow IAT Max'], flow_iat_mean=row['Flow IAT Mean'],\n",
    "                       flow_iat_min=row['Flow IAT Min'], flow_iat_std=row['Flow IAT Std'],\n",
    "                       fwd_iat_max=row['Fwd IAT Max'], fwd_iat_mean=row['Fwd IAT Mean'],\n",
    "                       fwd_iat_min=row['Fwd IAT Min'], fwd_iat_std=row['Fwd IAT Std'],\n",
    "                       bwd_iat_max=row['Bwd IAT Max'], bwd_iat_mean=row['Bwd IAT Mean'],\n",
    "                       bwd_iat_min=row['Bwd IAT Min'], bwd_iat_std=row['Bwd IAT Std'],\n",
    "                       active_max=row['Active Max'], active_mean=row['Active Mean'],\n",
    "                       active_min=row['Active Min'], active_std=row['Active Std'])\n",
    "            \n",
    "            # Context of Device and Environment Edge\n",
    "            G.add_edge(*edge_id, interaction='context',\n",
    "                       timestamp=idx, idle_max=row['Idle Max'],  # using idx here\n",
    "                       idle_mean=row['Idle Mean'], idle_min=row['Idle Min'], \n",
    "                       idle_std=row['Idle Std'])\n",
    "            \n",
    "            # Knowledge Graph Edge\n",
    "            G.add_edge(*edge_id, interaction='knowledge',\n",
    "                       down_up_ratio=row['Down/Up Ratio'], fwd_urg_flags=row['Fwd URG Flags'], bwd_urg_flags=row['Bwd URG Flags'])\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError in slice {index}: {e} - missing data fields.\")\n",
    "        return f\"KeyError in slice {index}: {e}\"\n",
    "\n",
    "    # Only save the graph if it contains any edges or nodes\n",
    "    if G.number_of_edges() > 0 or G.number_of_nodes() > 0:\n",
    "        filename = f\"graph_{interval}_{index}.pkl\"\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(G, f)\n",
    "        return filename\n",
    "    else:\n",
    "        return f\"No data to save for slice {index}\"\n",
    "\n",
    "# Continue with processing if no columns are missing\n",
    "if not missing_columns:\n",
    "    time_slices = [g for _, g in df.groupby(pd.Grouper(freq=time_interval))]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = []\n",
    "        for i, slice_df in enumerate(time_slices):\n",
    "            futures.append(executor.submit(process_time_slice, slice_df, time_interval, i))\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            print(f\"Saved: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9cc1ec-8313-4c65-9789-32ccccdf8b8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#graph creation for Test\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Ton_test_up.csv')\n",
    "\n",
    "# Convert 'Timestamp' to datetime and set as index\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "# Sort the DataFrame by the index (Timestamp)\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# Define time interval (Daily, hourly or per minutes ....)\n",
    "time_interval = 'X'\n",
    "\n",
    "# Required columns list including new features for edge attributes and labels\n",
    "required_columns = [\n",
    "    'Src IP', 'Dst IP', 'Flow ID', 'Src Port', 'Dst Port', 'Protocol',\n",
    "    'Flow Duration', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Flow IAT Max',\n",
    "    'Flow IAT Mean', 'Flow IAT Min', 'Flow IAT Std', 'Fwd IAT Max',\n",
    "    'Fwd IAT Mean', 'Fwd IAT Min', 'Fwd IAT Std', 'Bwd IAT Max',\n",
    "    'Bwd IAT Mean', 'Bwd IAT Min', 'Bwd IAT Std', 'Active Max',\n",
    "    'Active Mean', 'Active Min', 'Active Std', 'Idle Max', 'Idle Mean',\n",
    "    'Idle Min', 'Idle Std', 'Down/Up Ratio', 'Fwd URG Flags', 'Bwd URG Flags', 'Attack'\n",
    "]\n",
    "\n",
    "# Check for missing columns in the DataFrame\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"The following required columns are missing from the DataFrame: {missing_columns}\")\n",
    "else:\n",
    "    print(\"All required columns are present in the DataFrame.\")\n",
    "\n",
    "def process_time_slice(slice_df, interval, index):\n",
    "    G = nx.DiGraph()\n",
    "    labels = {}  # Dictionary to store labels by edge\n",
    "\n",
    "    try:\n",
    "        for idx, row in slice_df.iterrows():\n",
    "            if pd.isna(row['Src IP']) or pd.isna(row['Dst IP']):\n",
    "                continue\n",
    "\n",
    "            edge_id = (row['Src IP'], row['Dst IP'])\n",
    "\n",
    "            # Define default attributes common to all edges\n",
    "            common_attrs = {\n",
    "                'flow_id': row['Flow ID'], 'src_port': row['Src Port'], 'dst_port': row['Dst Port'],\n",
    "                'protocol': row['Protocol'], 'fwd_psh_flags': row['Fwd PSH Flags'],\n",
    "                'bwd_psh_flags': row['Bwd PSH Flags'], 'flow_iat_max': row['Flow IAT Max'],\n",
    "                'flow_iat_mean': row['Flow IAT Mean'], 'flow_iat_min': row['Flow IAT Min'],\n",
    "                'flow_iat_std': row['Flow IAT Std'], 'fwd_iat_max': row['Fwd IAT Max'],\n",
    "                'fwd_iat_mean': row['Fwd IAT Mean'], 'fwd_iat_min': row['Fwd IAT Min'],\n",
    "                'fwd_iat_std': row['Fwd IAT Std'], 'bwd_iat_max': row['Bwd IAT Max'],\n",
    "                'bwd_iat_mean': row['Bwd IAT Mean'], 'bwd_iat_min': row['Bwd IAT Min'],\n",
    "                'bwd_iat_std': row['Bwd IAT Std'], 'active_max': row['Active Max'],\n",
    "                'active_mean': row['Active Mean'], 'active_min': row['Active Min'],\n",
    "                'active_std': row['Active Std']\n",
    "            }\n",
    "\n",
    "            # Network communication edge\n",
    "            G.add_edge(*edge_id, **common_attrs, interaction='network_communication')\n",
    "\n",
    "            # Context edge\n",
    "            G.add_edge(*edge_id, **common_attrs, interaction='context',\n",
    "                       timestamp=idx.timestamp(), idle_max=row['Idle Max'],\n",
    "                       idle_mean=row['Idle Mean'], idle_min=row['Idle Min'],\n",
    "                       idle_std=row['Idle Std'])\n",
    "\n",
    "            # Knowledge graph edge\n",
    "            G.add_edge(*edge_id, **common_attrs, interaction='knowledge',\n",
    "                       down_up_ratio=row['Down/Up Ratio'], fwd_urg_flags=row['Fwd URG Flags'],\n",
    "                       bwd_urg_flags=row['Bwd URG Flags'])\n",
    "\n",
    "            # Store attack status for each edge\n",
    "            labels[edge_id + ('network_communication',)] = row['Attack']\n",
    "            labels[edge_id + ('context',)] = row['Attack']  # Assume same attack status or derived\n",
    "            labels[edge_id + ('knowledge',)] = row['Attack']  # Assume same attack status or derived\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError in slice {index}: {e} - row data: {row.to_dict()}\")\n",
    "        return f\"KeyError in slice {index}: {e}\"\n",
    "\n",
    "    # Only save the graph if it contains any edges or nodes\n",
    "    if G.number_of_edges() > 0 or G.number_of_nodes() > 0:\n",
    "        graph_filename = f\"graph_{interval}_{index}.pkl\"\n",
    "        with open(graph_filename, \"wb\") as gf:\n",
    "            pickle.dump((G, labels), gf)  # Save both graph and labels together\n",
    "        return graph_filename\n",
    "    else:\n",
    "        return f\"No data to save for slice {index}\"\n",
    "\n",
    "# Continue with processing if no columns are missing\n",
    "if not missing_columns:\n",
    "    time_slices = [g for _, g in df.groupby(pd.Grouper(freq=time_interval))]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = []\n",
    "        for i, slice_df in enumerate(time_slices):\n",
    "            futures.append(executor.submit(process_time_slice, slice_df, time_interval, i))\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if isinstance(result, str) and result.startswith(\"graph\"):\n",
    "                print(f\"Saved Graph and Labels: {result}\")\n",
    "            else:\n",
    "                print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c9d4a-84eb-493e-98ac-130fee188bc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#community detection for train\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from torch_geometric.utils import from_networkx\n",
    "import networkx as nx\n",
    "from tqdm.contrib.concurrent import process_map  # for progress bars with multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import random\n",
    "import time  # Import the time module\n",
    "\n",
    "def load_graph(graph_path):\n",
    "    print(f\"Loading graph from {graph_path}\")\n",
    "    with open(graph_path, 'rb') as f:\n",
    "        nx_graph = pickle.load(f)\n",
    "    return nx_graph\n",
    "\n",
    "def directed_label_propagation(G, max_iter=100):\n",
    "    # Initialize labels at random\n",
    "    labels = {n: i for i, n in enumerate(G.nodes())}\n",
    "    nodes = list(G.nodes())\n",
    "    random.shuffle(nodes)  # Random node order per iteration\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        converged = True  # To check if any label changed in this iteration\n",
    "        for node in nodes:\n",
    "            # Get the labels of the inbound neighbors\n",
    "            in_labels = [labels[n] for n in G.predecessors(node)]\n",
    "\n",
    "            if not in_labels:\n",
    "                continue\n",
    "\n",
    "            # Find the most common label\n",
    "            most_common = max(set(in_labels), key=in_labels.count)\n",
    "\n",
    "            # Update the label of the node if different\n",
    "            if labels[node] != most_common:\n",
    "                labels[node] = most_common\n",
    "                converged = False\n",
    "\n",
    "        if converged:\n",
    "            break  # Stop if no labels changed\n",
    "\n",
    "    return labels\n",
    "\n",
    "def detect_label_propagation(nx_graph):\n",
    "    print(\"Starting label propagation...\")\n",
    "    start_time = time.time()\n",
    "    labels = directed_label_propagation(nx_graph)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Creating communities from labels\n",
    "    communities = {}\n",
    "    for node, community_id in labels.items():\n",
    "        if community_id not in communities:\n",
    "            communities[community_id] = []\n",
    "        communities[community_id].append(node)\n",
    "\n",
    "    community_labels = labels\n",
    "    result = {\n",
    "        \"num_large_communities\": len([c for c in communities.values() if len(c) >= 3]),\n",
    "        \"duration\": end_time - start_time,\n",
    "        \"community_labels\": community_labels\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def add_node_features(G):\n",
    "    # Handle MultiGraph directly for degrees and simplify for other metrics\n",
    "    degrees = dict(G.degree())\n",
    "    simple_G = nx.Graph(G)\n",
    "    clustering = nx.clustering(simple_G)\n",
    "\n",
    "    for node in G.nodes():\n",
    "        G.nodes[node]['x'] = [\n",
    "            degrees.get(node, 0),\n",
    "            clustering.get(node, 0)\n",
    "        ]\n",
    "\n",
    "def ensure_edge_attributes(G, default_attributes):\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        for attr in default_attributes:\n",
    "            if attr not in data:\n",
    "                data[attr] = default_attributes[attr]\n",
    "\n",
    "def add_community_labels_to_nodes(G, community_labels):\n",
    "    for node in G.nodes():\n",
    "        G.nodes[node]['community'] = community_labels.get(node, -1)\n",
    "\n",
    "def process_graph(graph_path):\n",
    "    G = load_graph(graph_path)\n",
    "    community_result = detect_label_propagation(G)\n",
    "    add_community_labels_to_nodes(G, community_result['community_labels'])\n",
    "    add_node_features(G)\n",
    "    ensure_edge_attributes(G, default_attributes)\n",
    "\n",
    "    # Convert to PyTorch Geometric data\n",
    "    pyg_data = from_networkx(G)\n",
    "    pyg_data.x = torch.tensor([G.nodes[node]['x'] for node in G.nodes()], dtype=torch.float)\n",
    "    pyg_data.y = torch.tensor([G.nodes[node]['community'] for node in G.nodes()], dtype=torch.float)\n",
    "\n",
    "    # Save the processed graph\n",
    "    filename = f\"{os.path.basename(graph_path)[:-4]}.pt\"\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "    torch.save(pyg_data, save_path)\n",
    "    return save_path\n",
    "\n",
    "def main():\n",
    "    files = [os.path.join(graph_dir, f) for f in os.listdir(graph_dir) if f.endswith('.pkl')]\n",
    "    results = process_map(process_graph, files, max_workers=4, chunksize=1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    graph_dir = 'address of directory'\n",
    "    save_dir = 'address of directory'\n",
    "    default_attributes = {\n",
    "        'flow_id': None, 'src_port': None, 'dst_port': None, 'protocol': None,\n",
    "        'flow_duration': None, 'fwd_psh_flags': None, 'bwd_psh_flags': None,\n",
    "        'flow_iat_max': None, 'flow_iat_mean': None, 'flow_iat_min': None,\n",
    "        'flow_iat_std': None, 'fwd_iat_max': None, 'fwd_iat_mean': None,\n",
    "        'fwd_iat_min': None, 'fwd_iat_std': None, 'bwd_iat_max': None,\n",
    "        'bwd_iat_mean': None, 'bwd_iat_min': None, 'bwd_iat_std': None,\n",
    "        'active_max': None, 'active_mean': None, 'active_min': None,\n",
    "        'active_std': None, 'timestamp': None, 'idle_max': None,\n",
    "        'idle_mean': None, 'idle_min': None, 'idle_std': None,\n",
    "        'down_up_ratio': None, 'fwd_urg_flags': None, 'bwd_urg_flags': None,\n",
    "        'weight': 1.0\n",
    "    }\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a7044c-dd88-40e4-aee8-ac81cfd21622",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#community detection for test\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from torch_geometric.utils import from_networkx\n",
    "import networkx as nx\n",
    "from tqdm.contrib.concurrent import process_map  # for progress bars with multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import random\n",
    "import time  # Import the time module\n",
    "\n",
    "def load_graph(graph_path):\n",
    "    print(f\"Loading graph from {graph_path}\")\n",
    "    with open(graph_path, 'rb') as f:\n",
    "        nx_graph = pickle.load(f)\n",
    "    return nx_graph\n",
    "\n",
    "def directed_label_propagation(G, labels, max_iter=100):\n",
    "    # Initialize labels at random or use the provided labels\n",
    "    if not labels:\n",
    "        labels = {n: i for i, n in enumerate(G.nodes())}\n",
    "    nodes = list(G.nodes())\n",
    "    random.shuffle(nodes)  # Random node order per iteration\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        converged = True\n",
    "        for node in nodes:\n",
    "            in_labels = [labels[n] for n in G.predecessors(node) if n in labels]\n",
    "            if not in_labels:\n",
    "                continue\n",
    "            most_common = max(set(in_labels), key=in_labels.count)\n",
    "            if labels[node] != most_common:\n",
    "                labels[node] = most_common\n",
    "                converged = False\n",
    "        if converged:\n",
    "            break\n",
    "    return labels\n",
    "\n",
    "def detect_label_propagation(nx_graph, labels):\n",
    "    print(\"Starting label propagation...\")\n",
    "    start_time = time.time()\n",
    "    labels = directed_label_propagation(nx_graph, labels)\n",
    "    end_time = time.time()\n",
    "\n",
    "    communities = {}\n",
    "    for node, community_id in labels.items():\n",
    "        if community_id not in communities:\n",
    "            communities[community_id] = []\n",
    "        communities[community_id].append(node)\n",
    "\n",
    "    result = {\n",
    "        \"num_large_communities\": len([c for c in communities.values() if len(c) >= 3]),\n",
    "        \"duration\": end_time - start_time,\n",
    "        \"community_labels\": labels\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def process_graph(graph_path):\n",
    "    G, labels = load_graph(graph_path)\n",
    "    community_result = detect_label_propagation(G, labels)  # Correctly pass both graph and labels\n",
    "\n",
    "    add_community_labels_to_nodes(G, community_result['community_labels'])\n",
    "    add_node_features(G)\n",
    "    ensure_edge_attributes(G, default_attributes)\n",
    "\n",
    "    pyg_data = from_networkx(G)\n",
    "    pyg_data.x = torch.tensor([G.nodes[node]['x'] for node in G.nodes()], dtype=torch.float)\n",
    "    pyg_data.y = torch.tensor([G.nodes[node]['community'] for node in G.nodes()], dtype=torch.float) # Label of Community; used in training and test\n",
    "    pyg_data.z = torch.tensor([labels[(u, v, 'network_communication')] for u, v in G.edges()], dtype=torch.float) # Attack yes or no; used in prediction\n",
    "\n",
    "    filename = f\"{os.path.basename(graph_path)[:-4]}.pt\"\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "    torch.save(pyg_data, save_path)\n",
    "    return save_path\n",
    "\n",
    "def main():\n",
    "    files = [os.path.join(graph_dir, f) for f in os.listdir(graph_dir) if f.endswith('.pkl')]\n",
    "    results = process_map(process_graph, files, max_workers=4, chunksize=1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    graph_dir = 'address of directory'\n",
    "    save_dir = 'address of directory'\n",
    "    default_attributes = {\n",
    "        'flow_id': None, 'src_port': None, 'dst_port': None, 'protocol': None,\n",
    "        'flow_duration': None, 'fwd_psh_flags': None, 'bwd_psh_flags': None,\n",
    "        'flow_iat_max': None, 'flow_iat_mean': None, 'flow_iat_min': None,\n",
    "        'flow_iat_std': None, 'fwd_iat_max': None, 'fwd_iat_mean': None,\n",
    "        'fwd_iat_min': None, 'fwd_iat_std': None, 'bwd_iat_max': None,\n",
    "        'bwd_iat_mean': None, 'bwd_iat_min': None, 'bwd_iat_std': None,\n",
    "        'active_max': None, 'active_mean': None, 'active_min': None,\n",
    "        'active_std': None, 'timestamp': None, 'idle_max': None,\n",
    "        'idle_mean': None, 'idle_min': None, 'idle_std': None,\n",
    "        'down_up_ratio': None, 'fwd_urg_flags': None, 'bwd_urg_flags': None,\n",
    "        'weight': 1.0\n",
    "    }\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47857e1-c3b2-4cf2-81fb-6f5c9a277003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
