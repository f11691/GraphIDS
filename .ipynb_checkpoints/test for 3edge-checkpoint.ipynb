{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a18e4f4-fe4d-4031-b915-ca5bbea30b5b",
   "metadata": {},
   "source": [
    "# Create the test datastracture for 3 edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8b24475-be85-49be-bfd8-117211399d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3184195/2223271569.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['Timestamp'] = pd.to_datetime(filtered_data['Timestamp'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered and temporally ordered data saved to 'filtered_test_3edge.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the features to keep\n",
    "features_to_keep = [\n",
    "     #selected fetures\n",
    "]\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('test_data.csv')  # Replace 'test_data.csv' with your actual file name\n",
    "\n",
    "# Keep only the specified features\n",
    "filtered_data = data[features_to_keep]\n",
    "\n",
    "# Convert 'Timestamp' to datetime\n",
    "filtered_data['Timestamp'] = pd.to_datetime(filtered_data['Timestamp'])\n",
    "\n",
    "# Order the data by 'Timestamp'\n",
    "filtered_data = filtered_data.sort_values(by='Timestamp')\n",
    "\n",
    "# Save the temporally ordered data to a new file\n",
    "filtered_data.to_csv('filtered_test_3edge.csv', index=False)\n",
    "\n",
    "print(\"Filtered and temporally ordered data saved to 'filtered_test_3edge.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c76fb94-26af-487c-a5db-a891fb067c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution:\n",
      "Label\n",
      "1    825340\n",
      "0    540411\n",
      "Name: count, dtype: int64\n",
      "The CSV contains label '1'.\n"
     ]
    }
   ],
   "source": [
    "#check for inside of csv (just for test, no need for run)\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"filtered_test_3edge.csv\"  # Replace with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check if the label column contains '1'\n",
    "label_column = 'Label'  # Replace with the actual label column name if different\n",
    "if label_column in df.columns:\n",
    "    label_distribution = df[label_column].value_counts()\n",
    "    print(\"Label Distribution:\")\n",
    "    print(label_distribution)\n",
    "\n",
    "    if 1 in label_distribution.index:\n",
    "        print(\"The CSV contains label '1'.\")\n",
    "    else:\n",
    "        print(\"The CSV does NOT contain label '1'.\")\n",
    "else:\n",
    "    print(f\"'{label_column}' column not found in the CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb3f2e-b1a2-4884-ac70-c7bd008f0d42",
   "metadata": {},
   "source": [
    "# created hourly graph with 3 edges from test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c22f5e99-3e37-4dd6-9538-e18922186ab0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hour 0:\n",
      "Label\n",
      "1    239076\n",
      "0    146606\n",
      "Name: count, dtype: int64\n",
      "Test graph for hour 0 saved to 3ed_tes_h_graphs/test_graph_hour_0.gpickle\n",
      "Hour 1:\n",
      "Label\n",
      "1    372244\n",
      "0    225612\n",
      "Name: count, dtype: int64\n",
      "Test graph for hour 1 saved to 3ed_tes_h_graphs/test_graph_hour_1.gpickle\n",
      "Hour 2:\n",
      "Label\n",
      "1    214020\n",
      "0    168193\n",
      "Name: count, dtype: int64\n",
      "Test graph for hour 2 saved to 3ed_tes_h_graphs/test_graph_hour_2.gpickle\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "def create_test_graphs_edge_labels(df, output_dir):\n",
    "    \"\"\"\n",
    "    Split the DataFrame into hourly slices and create graphs for each slice.\n",
    "    Each edge gets a valid label (e.g., 0 or 1) read from the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame with temporal data.\n",
    "        output_dir (str): Directory to save the graphs.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Group the DataFrame into hourly slices using the datetime index.\n",
    "    # (Assumes the DataFrame index is already a DateTimeIndex)\n",
    "    time_slices = [g for _, g in df.groupby(pd.Grouper(freq='H'))]\n",
    "    \n",
    "    for slice_index, slice_df in enumerate(time_slices):\n",
    "        if slice_df.empty:\n",
    "            continue\n",
    "\n",
    "        # Print value counts of the 'Label' column in this time-slice.\n",
    "        print(f\"Hour {slice_index}:\")\n",
    "        print(slice_df['Label'].value_counts())\n",
    "        \n",
    "        # Create a MultiDiGraph for this time-slice.\n",
    "        G = nx.MultiDiGraph()\n",
    "\n",
    "        for _, row in slice_df.iterrows():\n",
    "            src_ip = row['Src IP']\n",
    "            dst_ip = row['Dst IP']\n",
    "            \n",
    "            # Convert the label to an int (if missing or invalid, you can decide a fallback; here we assume it is valid)\n",
    "            try:\n",
    "                label = int(row['Label'])\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping row due to invalid label: {row['Label']}; error: {e}\")\n",
    "                continue\n",
    "\n",
    "            if pd.isna(src_ip) or pd.isna(dst_ip):\n",
    "                continue\n",
    "\n",
    "            # Add nodes if not already present.\n",
    "            if not G.has_node(src_ip):\n",
    "                G.add_node(src_ip)\n",
    "            if not G.has_node(dst_ip):\n",
    "                G.add_node(dst_ip)\n",
    "\n",
    "            # Add edges for different interactions.\n",
    "            # 1. Network Edge\n",
    "            G.add_edge(src_ip, dst_ip, key='network',\n",
    "                       label=label,\n",
    "                  #selected fetures\n",
    "                       interaction='network_communication')\n",
    "\n",
    "            # 2. Context Edge\n",
    "            G.add_edge(src_ip, dst_ip, key='context',\n",
    "                       label=label,\n",
    "                      #selected fetures\n",
    "                       interaction='context')\n",
    "\n",
    "            # 3. Knowledge Edge\n",
    "            G.add_edge(src_ip, dst_ip, key='knowledge',\n",
    "                       label=label,\n",
    "                        #selected fetures\n",
    "                       interaction='knowledge')\n",
    "\n",
    "        # Save the graph as a .gpickle file.\n",
    "        graph_path = os.path.join(output_dir, f\"test_graph_hour_{slice_index}.gpickle\")\n",
    "        nx.write_gpickle(G, graph_path)\n",
    "        print(f\"Test graph for hour {slice_index} saved to {graph_path}\")\n",
    "\n",
    "# Usage Example for graph creation\n",
    "if __name__ == \"__main__\":\n",
    "    # Read CSV and prepare DataFrame.\n",
    "    df_test = pd.read_csv('filtered_test_3edge.csv')\n",
    "    df_test['Timestamp'] = pd.to_datetime(df_test['Timestamp'])\n",
    "    # Set Timestamp as index and sort (required for grouping by hour)\n",
    "    df_test = df_test.set_index('Timestamp').sort_index()\n",
    "\n",
    "    output_test_dir = \"3ed_tes_h_graphs\"\n",
    "    create_test_graphs_edge_labels(df_test, output_test_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a833c3e9-7ca1-46eb-9fc5-42434ec7fa05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hour 0:\n",
      "Label\n",
      "1    239076\n",
      "0    146606\n",
      "Name: count, dtype: int64\n",
      "Test graph for hour 0 saved to 3ed_tes_h_graphs/test_graph_hour_0.gpickle\n",
      "Hour 1:\n",
      "Label\n",
      "1    372244\n",
      "0    225612\n",
      "Name: count, dtype: int64\n",
      "Test graph for hour 1 saved to 3ed_tes_h_graphs/test_graph_hour_1.gpickle\n",
      "Hour 2:\n",
      "Label\n",
      "1    214020\n",
      "0    168193\n",
      "Name: count, dtype: int64\n",
      "Test graph for hour 2 saved to 3ed_tes_h_graphs/test_graph_hour_2.gpickle\n"
     ]
    }
   ],
   "source": [
    "#########Riplika of previous code:\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "def add_node_features(G):\n",
    "    \"\"\"\n",
    "    Adds additional features to nodes in the graph, including:\n",
    "    - Node degree\n",
    "    - Community ID\n",
    "    - Temporal activity (average edge count per node)\n",
    "    - Node centrality (betweenness centrality)\n",
    "\n",
    "    Parameters:\n",
    "        G (nx.MultiDiGraph): The input graph.\n",
    "\n",
    "    Returns:\n",
    "        nx.MultiDiGraph: The graph with added node features.\n",
    "    \"\"\"\n",
    "    # Add degree\n",
    "    for node in G.nodes:\n",
    "        G.nodes[node]['degree'] = G.degree[node]\n",
    "\n",
    "    # Add community detection (Label Propagation)\n",
    "    undirected_graph = nx.Graph(G)  # Convert to undirected for community detection\n",
    "    communities = nx.community.label_propagation_communities(undirected_graph)\n",
    "    community_mapping = {node: community_id for community_id, community in enumerate(communities) for node in community}\n",
    "    for node in G.nodes:\n",
    "        G.nodes[node]['community'] = community_mapping.get(node, -1)\n",
    "\n",
    "    # Add centrality (Betweenness Centrality)\n",
    "    centrality = nx.betweenness_centrality(G)\n",
    "    for node, value in centrality.items():\n",
    "        G.nodes[node]['centrality'] = value\n",
    "\n",
    "    return G\n",
    "\n",
    "def create_test_graphs_edge_labels(df, output_dir):\n",
    "    \"\"\"\n",
    "    Split the DataFrame into hourly slices and create graphs for each slice.\n",
    "    Each edge gets a valid label (e.g., 0 or 1) read from the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame with temporal data.\n",
    "        output_dir (str): Directory to save the graphs.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Group the DataFrame into hourly slices using the datetime index.\n",
    "    time_slices = [g for _, g in df.groupby(pd.Grouper(freq='H'))]\n",
    "    \n",
    "    for slice_index, slice_df in enumerate(time_slices):\n",
    "        if slice_df.empty:\n",
    "            continue\n",
    "\n",
    "        # Print value counts of the 'Label' column in this time-slice.\n",
    "        print(f\"Hour {slice_index}:\")\n",
    "        print(slice_df['Label'].value_counts())\n",
    "        \n",
    "        # Create a MultiDiGraph for this time-slice.\n",
    "        G = nx.MultiDiGraph()\n",
    "\n",
    "        for _, row in slice_df.iterrows():\n",
    "            src_ip = row['Src IP']\n",
    "            dst_ip = row['Dst IP']\n",
    "            \n",
    "            # Convert the label to an int (if missing or invalid, you can decide a fallback; here we assume it is valid)\n",
    "            try:\n",
    "                label = int(row['Label'])\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping row due to invalid label: {row['Label']}; error: {e}\")\n",
    "                continue\n",
    "\n",
    "            if pd.isna(src_ip) or pd.isna(dst_ip):\n",
    "                continue\n",
    "\n",
    "            # Add nodes if not already present.\n",
    "            if not G.has_node(src_ip):\n",
    "                G.add_node(src_ip)\n",
    "            if not G.has_node(dst_ip):\n",
    "                G.add_node(dst_ip)\n",
    "\n",
    "            # Add edges for different interactions.\n",
    "            # 1. Network Edge\n",
    "            G.add_edge(src_ip, dst_ip, key='network',\n",
    "                       label=label,\n",
    "                       #selected fetures\n",
    "                       interaction='network_communication')\n",
    "\n",
    "            # 2. Context Edge\n",
    "            G.add_edge(src_ip, dst_ip, key='context',\n",
    "                       label=label,\n",
    "                       #selected fetures\n",
    "                       interaction='context')\n",
    "\n",
    "            # 3. Knowledge Edge\n",
    "            G.add_edge(src_ip, dst_ip, key='knowledge',\n",
    "                       label=label,\n",
    "                       #selected fetures\n",
    "                       interaction='knowledge')\n",
    "\n",
    "        # Add node features\n",
    "        G = add_node_features(G)\n",
    "\n",
    "        # Save the graph as a .gpickle file.\n",
    "        graph_path = os.path.join(output_dir, f\"test_graph_hour_{slice_index}.gpickle\")\n",
    "        nx.write_gpickle(G, graph_path)\n",
    "        print(f\"Test graph for hour {slice_index} saved to {graph_path}\")\n",
    "\n",
    "# Usage Example for graph creation\n",
    "if __name__ == \"__main__\":\n",
    "    # Read CSV and prepare DataFrame.\n",
    "    df_test = pd.read_csv('filtered_test_3edge.csv')\n",
    "    df_test['Timestamp'] = pd.to_datetime(df_test['Timestamp'])\n",
    "    # Set Timestamp as index and sort (required for grouping by hour)\n",
    "    df_test = df_test.set_index('Timestamp').sort_index()\n",
    "\n",
    "    output_test_dir = \"3ed_tes_h_graphs\"\n",
    "    create_test_graphs_edge_labels(df_test, output_test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaf53ee-5666-494e-9284-b9a1d6b46c82",
   "metadata": {},
   "source": [
    "# Community detection for graphs and then update the graph with the label of community for each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bc85ed7-5dc9-4742-aef6-8a94f527ccbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated graph with LPA communities and 'x' attribute saved to 3ed_tes_h_graphs_commun/test_graph_hour_1.gpickle\n",
      "Updated graph with LPA communities and 'x' attribute saved to 3ed_tes_h_graphs_commun/test_graph_hour_2.gpickle\n",
      "Updated graph with LPA communities and 'x' attribute saved to 3ed_tes_h_graphs_commun/test_graph_hour_0.gpickle\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "def detect_and_label_communities_lpa(graph):\n",
    "    \"\"\"\n",
    "    Perform community detection using the Label Propagation Algorithm (LPA) and label nodes with community IDs.\n",
    "    Adds 'x' attribute based on the 'community' label.\n",
    "\n",
    "    Parameters:\n",
    "        graph (nx.MultiDiGraph): Input graph.\n",
    "\n",
    "    Returns:\n",
    "        graph (nx.MultiDiGraph): Updated graph with community labels and 'x' attributes.\n",
    "    \"\"\"\n",
    "    # Convert MultiDiGraph to Graph (undirected graph for LPA)\n",
    "    undirected_graph = nx.Graph(graph)\n",
    "\n",
    "    # Perform community detection using LPA\n",
    "    communities = nx.community.label_propagation_communities(undirected_graph)\n",
    "\n",
    "    # Assign community labels to nodes and add 'x' attribute\n",
    "    for community_id, community in enumerate(communities):\n",
    "        for node in community:\n",
    "            graph.nodes[node]['community'] = community_id\n",
    "            graph.nodes[node]['x'] = [community_id]  # 'x' is a feature; wrap in a list for PyTorch Geometric compatibility\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def process_graphs_with_lpa(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Detect communities using LPA, update graphs with community labels, and add 'x' attribute.\n",
    "    \n",
    "    Parameters:\n",
    "        input_dir (str): Directory containing input graphs.\n",
    "        output_dir (str): Directory to save updated graphs.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Process each graph file in the input directory\n",
    "    for graph_file in os.listdir(input_dir):\n",
    "        if not graph_file.endswith('.gpickle'):\n",
    "            continue\n",
    "        \n",
    "        # Load the graph\n",
    "        graph_path = os.path.join(input_dir, graph_file)\n",
    "        G = nx.read_gpickle(graph_path)\n",
    "\n",
    "        # Detect communities using LPA and label nodes\n",
    "        G = detect_and_label_communities_lpa(G)\n",
    "\n",
    "        # Save the updated graph\n",
    "        updated_graph_path = os.path.join(output_dir, graph_file)\n",
    "        nx.write_gpickle(G, updated_graph_path)\n",
    "        print(f\"Updated graph with LPA communities and 'x' attribute saved to {updated_graph_path}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Input directory containing graphs\n",
    "    input_graph_dir = \"3ed_tes_h_graphs\"\n",
    "\n",
    "    # Output directory for updated graphs\n",
    "    output_graph_dir = \"3ed_tes_h_graphs_commun\"\n",
    "\n",
    "    # Process graphs and add community labels using LPA\n",
    "    process_graphs_with_lpa(input_graph_dir, output_graph_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f3ceb3-8cf3-475a-8f7e-de2552cd1a64",
   "metadata": {},
   "source": [
    "# convert Multigraph to hetrodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "051672d2-b708-4835-9714-2c3ea1c585f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved HeteroData with labels to 3ed_tes_h_graphs_hetero_graphs/test_graph_hour_0.pt\n",
      "Saved HeteroData with labels to 3ed_tes_h_graphs_hetero_graphs/test_graph_hour_2.pt\n",
      "Saved HeteroData with labels to 3ed_tes_h_graphs_hetero_graphs/test_graph_hour_1.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "def multiDiGraph_to_hetero_with_label(G: nx.MultiDiGraph) -> HeteroData:\n",
    "    \"\"\"\n",
    "    Converts a MultiDiGraph with multiple edge types to a HeteroData object.\n",
    "    Preserves the 'label' field in data[rel_type].edge_label.\n",
    "    \"\"\"\n",
    "    data = HeteroData()\n",
    "    node_mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "    data['ip'].num_nodes = G.number_of_nodes()\n",
    "\n",
    "    # Add node-level features\n",
    "    x = []\n",
    "    community_labels = []\n",
    "    for node in G.nodes():\n",
    "        community = G.nodes[node].get('community', -1)\n",
    "        community_labels.append(community)\n",
    "        x.append([community])\n",
    "    data['ip'].community = torch.tensor(community_labels, dtype=torch.long)\n",
    "    data['ip'].x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "    # Process each edge from G.\n",
    "    for u, v, key, edge_attrs in G.edges(data=True, keys=True):\n",
    "        src = node_mapping[u]\n",
    "        dst = node_mapping[v]\n",
    "        rel_type = ('ip', key, 'ip')\n",
    "        if rel_type not in data.edge_types:\n",
    "            data[rel_type].edge_index = []\n",
    "            data[rel_type].edge_attr = []\n",
    "            data[rel_type].edge_label = []  # Container for the label\n",
    "\n",
    "        data[rel_type].edge_index.append([src, dst])\n",
    "        feature_vec = []\n",
    "        if key == 'network':\n",
    "            for attr_name in [ #selected fetures]:\n",
    "                feature_vec.append(edge_attrs.get(attr_name, 0))\n",
    "        elif key == 'context': #selected fetures]:\n",
    "                feature_vec.append(edge_attrs.get(attr_name, 0))\n",
    "        elif key == 'knowledge':\n",
    "            for attr_name in [ #selected fetures]:\n",
    "                feature_vec.append(edge_attrs.get(attr_name, 0))\n",
    "        data[rel_type].edge_attr.append(feature_vec)\n",
    "        # Save the label—this should now be valid (0 or 1)\n",
    "        data[rel_type].edge_label.append(edge_attrs.get('label', -1))\n",
    "\n",
    "    # Convert lists to tensors.\n",
    "    for rel_type in data.edge_types:\n",
    "        data[rel_type].edge_index = torch.tensor(data[rel_type].edge_index, dtype=torch.long).t().contiguous()\n",
    "        if data[rel_type].edge_attr:\n",
    "            data[rel_type].edge_attr = torch.tensor(data[rel_type].edge_attr, dtype=torch.float)\n",
    "        if data[rel_type].edge_label:\n",
    "            data[rel_type].edge_label = torch.tensor(data[rel_type].edge_label, dtype=torch.long)\n",
    "    return data\n",
    "\n",
    "def process_and_save_hetero_graphs_with_label(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Converts all .gpickle graphs in a directory to HeteroData objects and saves them as .pt,\n",
    "    preserving the 'label' field in data[rel_type].edge_label.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for graph_file in os.listdir(input_dir):\n",
    "        if not graph_file.endswith('.gpickle'):\n",
    "            continue\n",
    "        graph_path = os.path.join(input_dir, graph_file)\n",
    "        G = nx.read_gpickle(graph_path)\n",
    "        hetero_data = multiDiGraph_to_hetero_with_label(G)\n",
    "        hetero_path = os.path.join(output_dir, graph_file.replace('.gpickle', '.pt'))\n",
    "        torch.save(hetero_data, hetero_path)\n",
    "        print(f\"Saved HeteroData with labels to {hetero_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_test_dir = \"3ed_tes_h_graphs_commun\"         # Input .gpickle files (with communities added)\n",
    "    output_test_pt_dir = \"3ed_tes_h_graphs_hetero_graphs\" # Output .pt files\n",
    "    process_and_save_hetero_graphs_with_label(input_test_dir, output_test_pt_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32c7e39-ba5b-46e6-a122-1da2a430a570",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AnomalyDetection)",
   "language": "python",
   "name": "anomalydetection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
